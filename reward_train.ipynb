{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c52e9c0f",
   "metadata": {},
   "source": [
    "# Load dataset\n",
    "\n",
    "加载数据集，转换为trl reward trainer兼容的格式\n",
    "\n",
    "数据集转换的方式可以参考hf上的教程：https://huggingface.co/docs/trl/main/en/reward_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669dcccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict, load_dataset\n",
    "\n",
    "dataset_path = \"./COIG-P/data/*.parquet\"\n",
    "\n",
    "def make_conversation(example):\n",
    "    prompt = example.get(\"conversations\")[0][\"value\"]\n",
    "    chosen = example.get(\"chosen\")[\"value\"]\n",
    "    rejected = example.get(\"rejected\")[\"value\"]\n",
    "\n",
    "    return {\n",
    "        \"chosen\": [{\"role\": \"user\", \"content\": prompt}, {\"role\": \"assistant\", \"content\": chosen}],\n",
    "        \"rejected\": [{\"role\": \"user\", \"content\": prompt}, {\"role\": \"assistant\", \"content\": rejected}],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517f4e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 原始数据集\n",
    "dataset = load_dataset(\"parquet\", data_files=dataset_path)\n",
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadeb4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转换数据集\n",
    "dataset = dataset.map(make_conversation)\n",
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126a37fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分数据集\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=0.9, seed=42) # 只取50%用于训练\n",
    "split_dataset = dataset[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "dataset = DatasetDict({\n",
    "    \"train\": split_dataset[\"train\"],\n",
    "    \"test\": split_dataset[\"test\"]\n",
    "})\n",
    "print(dataset[\"train\"][0])\n",
    "print(dataset[\"test\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db355509",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1f6493",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments\n",
    "from trl import RewardConfig\n",
    "from peft import LoraConfig, TaskType\n",
    "\n",
    "# 加载模型\n",
    "model_path = \"./Qwen2.5-0.5B\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=1)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token  = tokenizer.eos_token\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# 加载配置\n",
    "training_args = RewardConfig(\n",
    "    output_dir=\"./Qwen2.5-0.5B-Reward\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=32,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=1.0e-3, # LoRA训练可以放大lr\n",
    "    warmup_ratio=0.1,\n",
    "    logging_first_step=True,\n",
    "    logging_steps=5,\n",
    "    logging_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "    gradient_checkpointing=True,\n",
    "    report_to=\"tensorboard\",\n",
    "    logging_dir=\"./reward_logs\",\n",
    "    bf16=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "# Lora配置\n",
    "peft_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    modules_to_save=[\"score\"],  # 很关键：保证 reward head 也能被保存/训练\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1636cc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from accelerate import logging\n",
    "from trl import RewardTrainer\n",
    "import peft\n",
    "\n",
    "trainer = RewardTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    peft_config=peft_config\n",
    ")\n",
    "print(trainer.args.device)\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"./Qwen2.5-0.5B-Reward\")\n",
    "tokenizer.save_pretrained(\"./Qwen2.5-0.5B-Reward\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ppo-reproduce",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
